#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import sys

from tensor2gan.models import hparams as HPARAMS
from tensor2gan.utils import registry
from tensor2gan.utils.save_image_hook import SaveImageHook
from tensor2gan.utils.builders import build_data_generator, build_model

import tensorflow as tf
flags = tf.flags
FLAGS = flags.FLAGS

# experiment settings
flags.DEFINE_string("model_dir", default_value='./results', 
    docstring="train results directory")
flags.DEFINE_string("data_dir", "./data", "train data directory")
flags.DEFINE_float("train_steps", 300000, "max number of training steps")
flags.DEFINE_bool("registry_help", False,
    "display contents of the registry and exit.")
flags.DEFINE_boolean('log_device_placement', False, 
    "Whether to log device placement.")

# eval steps
flags.DEFINE_integer('eval_freq', 10000, 'interval of evalution')
flags.DEFINE_integer('eval_size', 3, 'number of images to evaluate')

# model/save
flags.DEFINE_integer("save_freq", 5000, "save model every N steps")
flags.DEFINE_integer("keep_num_ckpts", 5, "keep max N checkpoints")

# model/dataset
flags.DEFINE_string("generator", "cifar10", 
    "Specify data_generator class. [cifar10|mnist|pokemon]")
flags.DEFINE_string("model", "DCGAN", 
    "Specify model class. [DCGAN|SN_DCGAN|WGAN]")

# hparams
flags.DEFINE_string("hparams_set", "dcgan_base", 
    "hparams_set. Default=dcgan_base")
flags.DEFINE_string("hparams", "", 
    "custom hparams, e.g. 'batch_size=32, z_dim=100'")

def get_data_generator():
    return build_data_generator(FLAGS.generator)

def create_hparams(hparams_set):
    hparams = getattr(HPARAMS, hparams_set)()
    hparams.parse(FLAGS.hparams)
    return hparams

def get_image_saver_hook(output_tensor):    
    image_dir = os.path.join(FLAGS.model_dir, "images")
    if not os.path.exists(image_dir):
        os.makedirs(image_dir)
    return SaveImageHook(
        output_tensor,
        save_steps=FLAGS.eval_freq,
        save_num=FLAGS.eval_size,
        image_dir=image_dir
        )

def log_registry():
    tf.logging.info("[log_registry]")
    if FLAGS.registry_help:
        tf.logging.info(registry.help_string())
        sys.exit(0)

def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)
    log_registry()

    # setup
    checkpoints_dir = FLAGS.model_dir
    if not os.path.exists(checkpoints_dir):
        os.makedirs(checkpoints_dir)

    # input     
    data_generator = get_data_generator()
    hparams = create_hparams(FLAGS.hparams_set)
    input_fn = data_generator.get_input_fn(
        batch_size=hparams.batch_size, 
        data_dir=FLAGS.data_dir, 
        train=True)

    # model config
    # TODO: refactor to namedtuple
    model_config = hparams
    model_config.add_hparam("input_shape", data_generator.input_shape)
    tf.logging.info("model_config: %s" % model_config)

    # graph setup
    graph = tf.Graph()
    with graph.as_default():

        step = tf.train.get_or_create_global_step()

        # input pipeline
        real_data = input_fn()
        # noise = tf.random_normal([hparams.batch_size, hparams.z_dim])
        noise = tf.random_uniform([hparams.batch_size, hparams.z_dim], 
            minval=-1, maxval=1)

        # model
        gan = build_model(FLAGS.model, model_config)
        losses, outputs, optimizers = gan.model([real_data, noise])

        # training summary/saver scaffold
        summary_op = tf.summary.merge_all()
        saver = tf.train.Saver(max_to_keep=FLAGS.keep_num_ckpts)
        scaffold = tf.train.Scaffold(
            summary_op=summary_op,
            saver=saver
        )

        # eval summary/saver scaffold
        fake_data = gan.eval([real_data, noise])
        eval_summary_op = tf.summary.merge_all("EVAL_SUMMARIES")
        eval_scaffold = tf.train.Scaffold(
            summary_op=eval_summary_op,
            saver=saver
        )

        hooks = [
            tf.train.StopAtStepHook(last_step=FLAGS.train_steps),
            tf.train.LoggingTensorHook(losses, every_n_iter=100),
            tf.train.CheckpointSaverHook(
                FLAGS.model_dir,
                save_steps=FLAGS.save_freq,
                scaffold=scaffold
            ),
            tf.train.SummarySaverHook(
                save_steps=100,
                scaffold=scaffold, # scaffold provides summary_op
                output_dir=FLAGS.model_dir
            ),
            tf.train.SummarySaverHook(
                save_steps=FLAGS.eval_freq,
                scaffold=eval_scaffold, # scaffold provides eval_summary_op
                output_dir=FLAGS.model_dir
            )]

        if FLAGS.eval_freq and FLAGS.eval_size:
            image_saver_hook = get_image_saver_hook(fake_data)
            hooks = hooks + [image_saver_hook]
            
        # train
        with tf.train.MonitoredTrainingSession(
            checkpoint_dir=FLAGS.model_dir,
            hooks=hooks,
            scaffold=scaffold,
            save_summaries_steps=None,
            save_summaries_secs=None,
            save_checkpoint_secs=None,
            config=tf.ConfigProto(
                log_device_placement=FLAGS.log_device_placement)) as mon_sess:
            
            while not mon_sess.should_stop():
                _updates = mon_sess.run(dict(
                    losses=losses, 
                    outputs=outputs,
                    optimizers=optimizers,
                    summary=[summary_op, eval_summary_op]
                ))


if __name__ == '__main__':
    tf.app.run()
    
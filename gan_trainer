#!/usr/bin/env python
# -*- coding: utf-8 -*-

import tensorflow as tf
import numpy as np
import os

from tensor2gan.data_generators.cifar10 import GenerateCIFAR10
from tensor2gan.data_generators.pokemon import GeneratePokemon

from tensor2gan.models.dcgan import DCGAN
from tensor2gan.models.sn_dcgan import SN_DCGAN
from tensor2gan.models import hparams as HPARAMS

flags = tf.flags
FLAGS = flags.FLAGS

# experiment settings
flags.DEFINE_string("model_dir", default_value='./results', 
    docstring="train results directory")
flags.DEFINE_string("data_dir", "./data", "train data directory")
flags.DEFINE_float("train_steps", 300000, "max number of training steps")

# model/save
flags.DEFINE_integer("save_freq", 5000, "save model every N steps")
flags.DEFINE_integer("keep_num_ckpts", 5, "keep max N checkpoints")

# model/dataset
flags.DEFINE_string("generator", "GenerateCIFAR10", 
    "Specify data_generator class. [GenerateCIFAR10|GeneratePokemon]")
flags.DEFINE_string("model", "DCGAN", 
    "Specify model class. [DCGAN|SN_DCGAN]")

# hparams
flags.DEFINE_string("hparams_set", "dcgan_base", 
    "hparams_set. Default=dcgan_base")
flags.DEFINE_string("hparams", "", 
    "custom hparams, e.g. 'batch_size=32, z_dim=100'")

def get_data_generator():
    class_name = FLAGS.generator
    _class = globals()[class_name]
    return _class()

def create_hparams(hparams_set):
    hparams = getattr(HPARAMS, hparams_set)()
    hparams.parse(FLAGS.hparams)
    return hparams

def main(_):
    # setup
    tf.logging.set_verbosity(tf.logging.INFO)
    checkpoints_dir = os.path.join(FLAGS.model_dir, "ckpts")
    if not os.path.exists(checkpoints_dir):
        os.makedirs(checkpoints_dir)

    # input     
    data_generator = get_data_generator()
    hparams = create_hparams(FLAGS.hparams_set)
    input_fn = data_generator.get_input_fn(
        batch_size=hparams.batch_size, 
        data_dir=FLAGS.data_dir, 
        train=True)

    # model config
    # TODO: refactor to namedtuple
    model_config = hparams
    model_config.add_hparam("input_shape", data_generator.input_shape)
    tf.logging.info("model_config: %s" % model_config)

    # graph setup
    graph = tf.Graph()
    with graph.as_default():
        # model
        model_name = FLAGS.model
        gan = globals()[model_name](model_config)

        # input pipeline
        real_data = input_fn()
        noise = tf.random_normal([hparams.batch_size, hparams.z_dim])
        losses, outputs, optimizers = gan.model([real_data, noise]) # returns dicts

        # summary/saver
        summary_op = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(checkpoints_dir, graph)
        saver = tf.train.Saver(max_to_keep=FLAGS.keep_num_ckpts)

    with tf.Session(graph=graph) as sess:
        # load checkpoint if available
        checkpoint = tf.train.get_checkpoint_state(checkpoints_dir)

        if checkpoint:
            meta_graph_path = checkpoint.model_checkpoint_path + ".meta"
            restore = tf.train.import_meta_graph(meta_graph_path)
            restore.restore(sess, tf.train.latest_checkpoint(checkpoints_dir))
            step = int(meta_graph_path.split("-")[1].split(".")[0])
        else:
            sess.run(tf.global_variables_initializer())
            step = 0

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)

        try:
            while not coord.should_stop():
                # train
                _updates = sess.run(dict(
                    losses=losses, 
                    outputs=outputs,
                    optimizers=optimizers,
                    summary=summary_op
                ))

                train_writer.add_summary(_updates['summary'], step)
                train_writer.flush()

                # logging
                if step % 100 == 0:
                    tf.logging.info("[step=%d] %s" %(step, _updates['losses']))

                # save model
                if step % FLAGS.save_freq == 0:
                    save_path = saver.save(sess, checkpoints_dir + "/model.ckpt", global_step=step)
                    if save_path:
                        tf.logging.info("[step=%d]   Model saved in file: %s" % (step, save_path))
                    else:
                        tf.logging.info("[step=%d]   error, model not saved." % step)

                # max training
                if (step + 1) % FLAGS.train_steps == 0:
                    tf.logging.info("*** end training, steps=%d *** " % step)
                    coord.request_stop()

                step += 1

        except KeyboardInterrupt:
            tf.logging.info('Interrupted')
            coord.request_stop()
        except Exception as e:
            coord.request_stop(e)
        finally:
            save_path = saver.save(sess, checkpoints_dir + "/model.ckpt", global_step=step)
            tf.logging.info("Model saved in file: %s" % save_path)
            # When done, ask the threads to stop.
            coord.request_stop()
            coord.join(threads)

if __name__ == '__main__':
    tf.app.run()
    
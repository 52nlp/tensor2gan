#!/usr/bin/env python
# -*- coding: utf-8 -*-

import tensorflow as tf
import numpy as np
import os

from tensor2gan.data_generators.cifar10 import GenerateCIFAR10
from tensor2gan.data_generators.pokemon import GeneratePokemon

from tensor2gan.models.dcgan import DCGAN
from tensor2gan.models.sn_dcgan import SN_DCGAN
from tensor2gan.models import hparams as HPARAMS

flags = tf.flags
FLAGS = flags.FLAGS

# experiment settings
flags.DEFINE_string("model_dir", default_value='./results', 
    docstring="train results directory")
flags.DEFINE_string("data_dir", "./data", "train data directory")
flags.DEFINE_float("train_steps", 300000, "max number of training steps")

flags.DEFINE_boolean('log_device_placement', False, 
    """Whether to log device placement.""")

# eval steps
flags.DEFINE_integer('eval_freq', 10000, 'interval of evalution')
flags.DEFINE_integer('eval_size', 5000, 'number of images to evaluate')

# model/save
# flags.DEFINE_integer("save_freq", 5000, "save model every N steps")
flags.DEFINE_integer("keep_num_ckpts", 5, "keep max N checkpoints")

# model/dataset
flags.DEFINE_string("generator", "GenerateCIFAR10", 
    "Specify data_generator class. [GenerateCIFAR10|GeneratePokemon]")
flags.DEFINE_string("model", "DCGAN", 
    "Specify model class. [DCGAN|SN_DCGAN]")

# hparams
flags.DEFINE_string("hparams_set", "dcgan_base", 
    "hparams_set. Default=dcgan_base")
flags.DEFINE_string("hparams", "", 
    "custom hparams, e.g. 'batch_size=32, z_dim=100'")

def get_data_generator():
    class_name = FLAGS.generator
    _class = globals()[class_name]
    return _class()

def create_hparams(hparams_set):
    hparams = getattr(HPARAMS, hparams_set)()
    hparams.parse(FLAGS.hparams)
    return hparams

def main(_):
    # setup
    tf.logging.set_verbosity(tf.logging.INFO)
    checkpoints_dir = FLAGS.model_dir
    if not os.path.exists(checkpoints_dir):
        os.makedirs(checkpoints_dir)

    # input     
    data_generator = get_data_generator()
    hparams = create_hparams(FLAGS.hparams_set)
    input_fn = data_generator.get_input_fn(
        batch_size=hparams.batch_size, 
        data_dir=FLAGS.data_dir, 
        train=True)

    # model config
    # TODO: refactor to namedtuple
    model_config = hparams
    model_config.add_hparam("input_shape", data_generator.input_shape)
    tf.logging.info("model_config: %s" % model_config)

    # graph setup
    graph = tf.Graph()
    with graph.as_default():

        step = tf.train.get_or_create_global_step()

        # model
        model_name = FLAGS.model
        gan = globals()[model_name](model_config)

        # input pipeline
        real_data = input_fn()
        noise = tf.random_normal([hparams.batch_size, hparams.z_dim])
        losses, outputs, optimizers = gan.model([real_data, noise]) # returns dicts

        # summary/saver
        summary_op = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(checkpoints_dir, graph)
        saver = tf.train.Saver(max_to_keep=FLAGS.keep_num_ckpts) # TODO: deprecate

        # hooks -- MonitoredTrainingSession summary saver doesn't seem to work. 
        hooks = [
            tf.train.StopAtStepHook(last_step=FLAGS.train_steps),
            tf.train.LoggingTensorHook(losses, every_n_iter=100),
            tf.train.SummarySaverHook(
                save_steps=100,
                summary_writer=train_writer,
                summary_op=summary_op
                )
            ]

        # train
        with tf.train.MonitoredTrainingSession(
            checkpoint_dir=FLAGS.model_dir,
            hooks=hooks,
            save_summaries_steps=None,
            config=tf.ConfigProto(
                log_device_placement=FLAGS.log_device_placement)) as mon_sess:
            
            while not mon_sess.should_stop():
                _updates = mon_sess.run(dict(
                    losses=losses, 
                    outputs=outputs,
                    optimizers=optimizers,
                    summary=summary_op,
                ))

if __name__ == '__main__':
    tf.app.run()
    